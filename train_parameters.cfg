# Change only thd right side of the "=" sign to change the value of the hyperparameter 
# Set the number of classes - including 1 background class
n_classes = 4
#Set base filters - inital number of filters and subsequently all the filters are multiples of this 
base_filters = 30
#Number of channels/modalities that are being fed into the network 
n_channels = 5
#Number of epochs
num_epochs = 350
#Choose the segmentation model here - either unet, resunet fcn, uinc - here the model is chosen by looking into the new_models.py file which inturn looks in the seg_module.py file where the actual architectiure is defined
which_model = resunet
#Path where the model has to be saved 
model_path = /cbica/home/bhaleram/comp_space/fets/model/ResUNet/Exp_3/
#Set the batch size
batch = 1
#Set the initial learning rate (actually ignore this parameter for now - the explnation
#will be done later - this is in close connection to how tne lr scheduler is designed
learning_rate = 1
#Set which ground truth you want to use - 0 for non overlap (original brats labels) and 1 for overlap (WT, TC, ET)
which_gt = 0
#Set the log interval i.e. after how many training examples u want to print training loss
log_t = 8
#Set the log interval i.e. after how many training examples u want to print training loss
log_v = 6
# Set which loss function you want to use - options : 'dc' - for dice only, 'dcce' - for sum of dice and CE and you can guess the next (only lower-case please)
which_loss = dc
# Which optimizer do you want to use - adam/sgd
opt = sgd
#How many best models to save
save_best = 5
